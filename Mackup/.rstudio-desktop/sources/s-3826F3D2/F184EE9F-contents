#install.packages("dplyr") #Comment out if already installed
#install.packages("dagitty") #Comment out if already installed
#install.packages("Hmisc")
#install.packages("caTools")
#install.packages("naivebayes")
#install.packages("bnlearn")
#install.packages("pROC")
library(dplyr)
library(dagitty)
library(Hmisc)
library(caTools)



#Load data
setwd("/Users/max/Google Drive/1 Uni/1.1 Kurse/Baysian Networks/Projekt")
# load csv
#all_df <- read.csv("data/all_processed.csv",header=TRUE,stringsAsFactors=FALSE) #colClasses=rep("double",4),
d <- read.csv("data/dataCleveland.csv",header=TRUE,colClasses=rep("double",4),stringsAsFactors=FALSE)
head(d)

#sd <- select(d,"cigs")


#Select right variables
subset <- select(d, 3,4,9,16,10,12,14,15,44,51,58)

#Filter out mising values
prep <- filter(subset, age != -9 &  sex != -9 & cp != -9 &fbs != -9 & trestbps!= -9 & chol != -9 &cigs != -9 &years!= -9 & ca != -9 &thal != -9 &num!= -9 )
nrow(prep)
nrow(subset)

#Plot data continous data 
#hist(prep$age)
#hist(prep$chol)
#hist(prep$cigs)
#hist(prep$years)
#hist(prep$trestbps)

#Bin continous data into categorical data of 3 bins.
amountBins <- 3

continuousVars <- list("age", "chol", "cigs", "years","trestbps")
continuousVars <- unlist(continuousVars)
for(i in 1:length(continuousVars)){
  varname <- continuousVars[i]
  intervalValues.varname <- list()
  intervals.varname <-  names(split(prep, cut2(prep[[varname]], g=amountBins)))
  
  for(j in 1:length(intervals.varname)){
    
    intervals.t <-  unlist(strsplit(intervals.varname[j], ','))
    intervals.fst <- intervals.t[1]
    intervals.snd <- intervals.t[2]
    nc.1 <- nchar(intervals.fst)
    nc.2 <- nchar(intervals.snd)
    intervalValues.varname <- append(intervalValues.varname, as.numeric(substr(intervals.fst,2,nc.1)))
    intervalValues.varname <- append(intervalValues.varname, as.numeric(substr(intervals.snd,1,nc.2-1)))
    
  }
  intervalValues.varname <- unique(unlist(intervalValues.varname))
  intervalValues.varname <- intervalValues.varname[!is.na(intervalValues.varname)]
  intervalValues.varname[1] = intervalValues.varname[1]-1
  intervalValues.varname[length(intervalValues.varname)] = intervalValues.varname[length(intervalValues.varname)]+1
  
  colname.D <-  paste(varname, ".discrete",sep="")
  colname.N <- paste(varname, ".numeric", sep = "")
  prep[colname.D] <- cut( prep[[varname]], breaks=c(intervalValues.varname))
  prep[colname.N] <- as.numeric( prep[[colname.D]])
}

#### make target variable binary #####
## 0 --> 0 & 1,2,3 --> 1 #######
num.binary <- prep$num
prep$num.binary <- rapply(as.list(num.binary), function(x) ifelse(x!=0, 1,x), how = "replace")


########################################################################################
female <- select((prep[prep$sex ==0 , ]), sex=0, age.numeric)
female$sex <- "female"

male <- select((prep[prep$sex ==1 , ]), sex=0, age.numeric)
male$sex <- "male"






####################################################################
## Create training and test set ##
##################################
set.seed(13)

#Select only variables we need (numeric)
data <- select(prep, 2,3,4,9,10,11,13,15,17,19,21)
data.binary <- select(prep, 2,3,4,9,10,13,15,17,19,21)#, 22)
data.binary$num.binary <- as.double(prep[,22])

# split the data 
smp_size = floor(0.75 *  nrow(data))
train_ind <- sample(seq_len(nrow(data)), size = smp_size)
train <-  data[train_ind,]
test <- data[-train_ind,]
train.binary <-  data.binary[train_ind,]
test.binary <- data.binary[-train_ind,]

train_neg = sum(train$num.binary==0)
train_pos = sum(train$num.binary==1)
test_neg = sum(test$num.binary==0)
test_pos = sum(test$num.binary==1)
train_ratio = train_pos/train_neg
test_ratio = test_pos/test_neg

####################################################################
##### Auto Lean Model #######

library(bnlearn)
data(learning.test)
data(gaussian.test)

learn.net = empty.graph(names(learning.test))


